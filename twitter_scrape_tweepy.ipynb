{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#import code.auth_and_scrape as get_tweets\n",
    "#import code.preparation as prep\n",
    "#import code.visualization as viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter API Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "consumerKey = 'API Key'\n",
    "consumerSecret = 'API Secret'\n",
    "accessToken = 'Acccess Token'\n",
    "accessTokenSecret = 'Access Secret'\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To CSV Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(tweets, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in tweets:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. ID\n",
    "        id = tweet.id\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = tweet.created_at\n",
    "\n",
    "        # 4. Language\n",
    "        lang = tweet.lang\n",
    "\n",
    "        # 5. Tweet metrics\n",
    "        retweet_count = tweet.retweet_count\n",
    "        like_count = tweet.favorite_count\n",
    "\n",
    "        # 6. Tweet text\n",
    "        text = tweet.text\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [id, created_at, lang, like_count, retweet_count, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scrape a chosen amount of tweets with a chosen keyword and append to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_place():\n",
    "    #Ask for keyword with which to scrape.\n",
    "    keyword = input('Please enter keyword or hastag to search: ')\n",
    "    #Ask for number of tweets desired to scrape.\n",
    "    tweet_num = int(input('Please enter how many tweets to analyze (If greater than 900, must be multiple of 900): '))\n",
    "    #Ask for date at which access was requested as the scrape can only occur on tweets made within seven days prior to requested date.\n",
    "    date_req = input('Please enter the date from which you request YYYY/MM/DD: ')\n",
    "    date_req = date_req.split('/')\n",
    "    j = 0\n",
    "    #Only 900 tweets can be scraped every 15min so if desired amount is greater than 900 this will create a wait time to pull every 15 min.\n",
    "    if tweet_num > 900:\n",
    "        for i in range(0, tweet_num//900):\n",
    "            if j == 7:\n",
    "                break\n",
    "            elif j != 7:\n",
    "                tweets = tweepy.Cursor(api.search_tweets, q=keyword+' lang:en -filter:retweets', until=date_req[0] + '-' + date_req[1] + '-' + str((int(date_req[2]) - j))).items(900)\n",
    "            j += 1\n",
    "            append_to_csv(tweets, fileName='data.csv')\n",
    "            time.sleep(900)\n",
    "\n",
    "    elif tweet_num <= 900 & tweet_num > 0:\n",
    "        for i in range(0, tweet_num):\n",
    "            tweets = tweepy.Cursor(api.search_tweets, q=keyword+' lang:en -filter:retweets', until=date_req[0] + '-' + date_req[1] + '-' + date_req[2]).items(tweet_num)\n",
    "            append_to_csv(tweets, fileName='data.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Tweets added from this response:  900\n",
      "# of Tweets added from this response:  900\n",
      "# of Tweets added from this response:  900\n",
      "# of Tweets added from this response:  900\n",
      "# of Tweets added from this response:  900\n",
      "# of Tweets added from this response:  900\n",
      "# of Tweets added from this response:  900\n"
     ]
    }
   ],
   "source": [
    "scrape_and_place()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>language</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1462933594754670600</td>\n",
       "      <td>2021-11-22 23:58:43+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@DestinyTheGame all id like is to be at least ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462933098887266308</td>\n",
       "      <td>2021-11-22 23:56:45+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@DestinyTheGame Can someone please explain to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1462932969086087176</td>\n",
       "      <td>2021-11-22 23:56:14+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@techaftmidnight @GoogleStadia @DestinyTheGame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1462932844418842632</td>\n",
       "      <td>2021-11-22 23:55:44+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Going live in 5 minutes!! Let's go GOD SQUAD!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1462932760515985420</td>\n",
       "      <td>2021-11-22 23:55:24+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@A_dmg04 @DestinyTheGame Dear Eva, please brin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                 created_at language  favorites  \\\n",
       "0  1462933594754670600  2021-11-22 23:58:43+00:00       en          0   \n",
       "1  1462933098887266308  2021-11-22 23:56:45+00:00       en          0   \n",
       "2  1462932969086087176  2021-11-22 23:56:14+00:00       en          1   \n",
       "3  1462932844418842632  2021-11-22 23:55:44+00:00       en          1   \n",
       "4  1462932760515985420  2021-11-22 23:55:24+00:00       en          0   \n",
       "\n",
       "   retweets                                               text  \n",
       "0         0  @DestinyTheGame all id like is to be at least ...  \n",
       "1         0  @DestinyTheGame Can someone please explain to ...  \n",
       "2         0  @techaftmidnight @GoogleStadia @DestinyTheGame...  \n",
       "3         0  Going live in 5 minutes!! Let's go GOD SQUAD!!...  \n",
       "4         0  @A_dmg04 @DestinyTheGame Dear Eva, please brin...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6300.000000\n",
       "mean        9.520635\n",
       "std        90.900910\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         1.000000\n",
       "75%         2.000000\n",
       "max      3879.000000\n",
       "Name: favorites, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.favorites.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@DestinyTheGame Can someone please explain to me why I (top 7-10%ish player) keep getting teamed with people in the… https://t.co/fREoFm7Emb'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tweet has a ton of punctuation and it even includes a url. In order for our sentiment analyzer to read and understand this, we must remove the all of this and even lowercase every letter. Here A function has been create to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean tweets of urls, punctuation and uppercase letters\n",
    "def clean_tweets(dataframe):\n",
    "\n",
    "    #Lambda function to remove urls from tweets providing links\n",
    "    remove_url = lambda tweet: re.sub(r'http\\S+', '', tweet)\n",
    "\n",
    "    #Lambda function to remove punctuation from tweets\n",
    "    remove_punct = lambda tweet: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \", tweet)\n",
    "\n",
    "    #Remove urls and punctuation from text column\n",
    "    dataframe[\"clean_text\"] = dataframe.text.map(remove_url).map(remove_punct)\n",
    "\n",
    "    #Reduce text to lowercase format\n",
    "    dataframe[\"clean_text\"] = dataframe.clean_text.str.lower()\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_tweets(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to create a Sentiment Analysis using TextBlob and SentimentIntensityAnalyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound \n",
    "def create_sentiment(dataframe):\n",
    "    df_probs = pd.DataFrame()\n",
    "\n",
    "    #create dataframe of subjectivity and subjectivity polarity of tweets\n",
    "    df_probs[['polarity', 'subjectivity']] = dataframe['clean_text'].apply(lambda text: pd.Series(TextBlob(text).sentiment))\n",
    "\n",
    "    #Scores each tweet based on the subjectivity polarity\n",
    "    for index, row in dataframe['text'].iteritems():\n",
    "\n",
    "        #Provides negative, neutral, positive and compound scores for the tweet in that row\n",
    "        score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "\n",
    "        #extracts negative and positive scores\n",
    "        neg = score['neg']\n",
    "        pos = score['pos']\n",
    "        \n",
    "        #creates binary values based on which sentiment score is higher \n",
    "        if neg > pos:\n",
    "            df_probs.loc[index, 'sentiment'] = 0\n",
    "        elif pos > neg:\n",
    "            df_probs.loc[index, 'sentiment'] = 1\n",
    "        else:\n",
    "            df_probs.loc[index, 'sentiment'] = 0\n",
    "            \n",
    "    return df_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs = create_sentiment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering Positive Reinforcement Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dataframe of the positive tweets and matches their sentiment values\n",
    "def create_pos_df(dataframe, dataframe_probs):\n",
    "\n",
    "    #Grabs text from original dataframe \n",
    "    dataframe.reset_index(drop=True, inplace=True)\n",
    "    df_pos = df[['text']].copy(deep=True)\n",
    "\n",
    "    #Grabs sentiment values from probs dataframe\n",
    "    dataframe_probs.reset_index(drop=True, inplace=True)\n",
    "    df_pos['sentiment'] = df_probs['sentiment'].copy(deep=True)\n",
    "\n",
    "    #Creates dataframe of only positive sentiment tweets\n",
    "    df_pos = df_pos[df_pos['sentiment'] == 1]\n",
    "    \n",
    "    return df_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_uncleaned = create_pos_df(df, df_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_cleaned = clean_tweets(df_pos_uncleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "\n",
    "    #Vectorize corpus\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english', max_df=1.0, min_df=0.01).fit(corpus)\n",
    "\n",
    "    #creates bag of words and finds the sums of words\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    \n",
    "    #creates a list of words and their frequencies\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return words_freq[:n], bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['destiny2', 'threadsoflight', 'game', 'play', 'congrats', 'thanks', 'bring', 'destinythegame', 'make', 'really', 'don', 'hope', 'great', 'time', 'nice', 'better', 'new', 'day', 'playing', 'looks', 'congratulations', 'yeah', 'bungie', 'know', 'happy', 'trials', 'help', 'awesome', 'think', 've', 'going', 'dawning', 'wish', 'year', 'fun', 'got', 'space', 'hunter', 'people', 'shader', 'look', 'best', 'd2', 'live', 'crow', 'grandma', 'free', 'let', 'need', 'amazing', 'today', 'twab', 'friends', 'll', 'guardian', 'lol', 'week', 'use', 'getting', 'god', 'cool', 'pvp', 'changes', 'work', 'come', 'double', 'amp', 'right', 'looking', 'titan', 'team', 'maybe', 'ability', 'actually', 'oh', 'win', 'vex', 'pretty', 'flawless', 'thememe', 'super', 'man', 'played', 'rewards']\n"
     ]
    }
   ],
   "source": [
    "positives_and_count, pos_vec = get_top_n_gram(df_pos_cleaned['clean_text'], (1,1), 300)\n",
    "\n",
    "positives = [x[0] for x in positives_and_count]\n",
    "top_10_pos = positives[:10]\n",
    "positives_minus_top_10 = [x for x in positives if x not in top_10_pos]\n",
    "print(positives_minus_top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, top_words):\n",
    "    features = {}\n",
    "    wordcount = 0\n",
    "\n",
    "    #Adds 1 to wordcount if word in tokenized text is within the top list of words\n",
    "    for sentence in sent_tokenize(text):\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word.lower() in top_words:\n",
    "                wordcount += 1\n",
    "\n",
    "    #adds wordcount to dictionary\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features['wordcount']\n",
    "\n",
    "features = [\n",
    "    (extract_features(review, positives_minus_top_10))\n",
    "    for review in df['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.concat([df['clean_text'], pd.DataFrame(features,columns=['positive_word_count'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>positive_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>read my mind</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>oh god please no more cookie baking please</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4584</th>\n",
       "      <td>ahh   thanks so much  ghaul  lt 3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3768</th>\n",
       "      <td>these cool down changes in combination with da...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>they talking about why eris had to put her r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>getting back into sniping   slowly         des...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>congratulations    hell yeah</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>ab you have been having problems for years...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>dear eva   could you kindly ask ada 1 to sel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>true  i haven t seen one either   i m exci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4725 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text  positive_word_count\n",
       "4319                                       read my mind                    0\n",
       "518         oh god please no more cookie baking please                     2\n",
       "4584                  ahh   thanks so much  ghaul  lt 3                    1\n",
       "3768  these cool down changes in combination with da...                    2\n",
       "2990    they talking about why eris had to put her r...                    1\n",
       "...                                                 ...                  ...\n",
       "905   getting back into sniping   slowly         des...                    3\n",
       "5192                      congratulations    hell yeah                     2\n",
       "3980      ab you have been having problems for years...                    0\n",
       "235     dear eva   could you kindly ask ada 1 to sel...                    0\n",
       "5157      true  i haven t seen one either   i m exci...                    0\n",
       "\n",
       "[4725 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_feat, df_probs['sentiment'],random_state=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_gram_test(corpus, trained_corpus, ngram_range,n=None):\n",
    "\n",
    "    #Vectorize corpus\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english', max_df=1.0, min_df=0.01).fit(trained_corpus)\n",
    "\n",
    "    #Create bag of words and the sums\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "\n",
    "    #Create a list of words and their frequencies\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n], bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat, feat_vec_train = get_top_n_gram(X_train['clean_text'], (1,1), 100)\n",
    "feat, feat_vec_test = get_top_n_gram_test(X_test['clean_text'], X_train['clean_text'], (1,1), 100)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Create X_train vectorized\n",
    "X_train_vec = pd.concat([X_train['positive_word_count'], pd.DataFrame(feat_vec_train.todense())], axis=1)\n",
    "X_test_vec = pd.concat([X_test['positive_word_count'], pd.DataFrame(feat_vec_test.todense())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model is a simple logistic regression that scored 78% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7873015873015873"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train_vec, y_train)\n",
    "log_model.score(X_test_vec, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c94be41889154d41bf43aca8d1a8d1cd64b97c119170e03e2ed46ca87183f0c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
